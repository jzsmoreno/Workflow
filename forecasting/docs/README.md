## ¬øQu√© son las Redes Neuronales Recurrentes (RNN)?

Las **Redes Neuronales Recurrentes** son un tipo de arquitectura de redes neuronales dise√±ada espec√≠ficamente para trabajar con **secuencias de datos**, como:

- Texto
- Series temporales
- Se√±ales de audio o video

A diferencia de las redes convolucionales (CNN) y las redes feedforward, las RNN tienen un mecanismo de _memoria_ que les permite recordar informaci√≥n procesada en pasos anteriores. Es decir, **el estado interno** de la red se mantiene entre los elementos de la secuencia.

## Arquitectura b√°sica de una RNN simple

La **RNN simple** funciona con la idea de un bucle temporal: el output en cada paso se convierte en parte del input para el siguiente paso. Su estructura es:

$$h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$

Donde:

- $h_t$: estado oculto en el tiempo $t$
- $x_t$: entrada en el tiempo $t$
- $f$: funci√≥n de activaci√≥n (ej. $\tanh$ o ReLU)

### Ejemplo visual:

Matem√°ticamente, sup√≥ngase que se tienen $h$ unidades ocultas, tama√±o de batch $n$, y el n√∫mero de entradas es $d$. Entonces, la entrada es $x_{t} \in \mathbb{R}^{d\times n}$ y el estado oculto del paso anterior es $h_{t-1} \in \mathbb{R}^{h\times n}$.

$$
x_{t} = \left(\begin{matrix}
x^{1,1}_{t} & \cdots & x^{1,n}_{t} \\
\vdots & \ddots & \vdots \\
x^{d,1}_{t} & \cdots & x^{d,n}_{t}
\end{matrix}\right)
$$

donde $W_{hh} \in \mathbb{R}^{h\times h}$, $W_{xh} \in \mathbb{R}^{h\times d}$ y $b_h \in \mathbb{R}^{h\times 1}$, y el estado oculto al tiempo $t$ para $d = 1$ (caso de series temporales univariable) es calculado como le sigue:

$$
h_{t} =
f\left(
\left(\begin{matrix}
w^{1,1}_{hh} & \cdots & w^{1,h}_{hh} \\
\vdots & \ddots & \vdots \\
w^{h,1}_{hh} & \cdots & w^{h,h}_{hh}
\end{matrix}\right)
\left(\begin{matrix}
h^{1,1}_{t-1} & \cdots & h^{1,n}_{t-1} \\
\vdots & \ddots & \vdots \\
h^{h,1}_{t-1} & \cdots & h^{h,n}_{t-1}
\end{matrix}\right)
+
\left(\begin{matrix}
w^{1,1}_{xh} x^{1,1}_{t} & \cdots & w^{1,1}_{xh} x^{1,n}_{t} \\
\vdots & \ddots & \vdots \\
w^{h,1}_{xh} x^{1,1}_{t} & \cdots & w^{h,1}_{xh} x^{1,n}_{t}
\end{matrix}\right)
+
\left(\begin{matrix}
b^{1}_{h} \\
\vdots \\
b^{h}_{h}
\end{matrix}\right)
\right)
$$

## Limitaciones de las RNN simples

- **Problema de gradientes desaparecientes/estallantes**: No pueden manejar bien dependencias a largo plazo (ej. en un texto, no "recuerdan" lo que sucedi√≥ muchas palabras atr√°s).

## LSTM: Long Short-Term Memory

La **LSTM** es una versi√≥n mejorada de la RNN simple. Introduce mecanismos para controlar qu√© informaci√≥n se retiene y cu√°l se olvida.

### Componentes principales:

- **Puerta de olvido (Forget Gate)** ‚Üí Decide qu√© informaci√≥n del estado previo debe desecharse.
- **Puerta de entrada (Input Gate)** ‚Üí Decide nuevas celdas a almacenar en la memoria.
- **Celda de memoria (Cell State)** ‚Üí Mantiene informaci√≥n por un largo tiempo.
- **Puerta de salida (Output Gate)** ‚Üí Calcula el output basado en la celda actual.

### ‚öôÔ∏è Ecuaciones resumidas:

1. $f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$
2. $i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$
3. $\tilde{C}_{t} = \tanh(W_C [h_{t-1}, x_t] + b_C)$
4. $C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$
5. $o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)$
6. $h_t = o_t * \tanh(C_t)$

Donde $\sigma$ es la funci√≥n sigmoide, y $*$ es multiplicaci√≥n elemento a elemento.

### Ventajas:

- Maneja dependencias largas.
- Es m√°s estable num√©ricamente que las RNN simples.

---

## GRU: Gated Recurrent Unit

La **GRU** tambi√©n resuelve el problema de los gradientes, pero con una estructura ligeramente m√°s sencilla que la LSTM. Combina las **puertas de olvido y entrada en una sola puerta (z)**, adem√°s de fusionar el estado oculto y la celda.

### üîß Componentes principales:

- **Puerta de actualizaci√≥n (Update Gate):** Decide qu√© informaci√≥n actualizar.
- **Puerta de restablecimiento (Reset Gate):** Controla cu√°nto del pasado afectar√° a la nueva entrada.

### üìò Ecuaciones resumidas:

1. $z_t = \sigma(W_z [h_{t-1}, x_t] + b_z)$
2. $r_t = \sigma(W_r [h_{t-1}, x_t] + b_r)$
3. $\tilde{h}_{t}= \tanh(W_h [r_t * h_{t-1}, x_t] + b_h)$
4. $h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$

### Ventajas:

- M√°s ligera que la LSTM.
- Menos par√°metros ‚Üí entrenamiento m√°s r√°pido en algunos casos.

---

## Comparaci√≥n r√°pida

| Modelo   | Memoria a largo plazo | Capacidad de modelado | Velocidad / Tama√±o               |
| -------- | --------------------- | --------------------- | -------------------------------- |
| **RNN**  | ‚ùå Limitada           | Baja                  | ‚ö° R√°pida pero inestable         |
| **LSTM** | ‚úÖ Alta               | Alta                  | üê¢ M√°s lenta, m√°s precisa        |
| **GRU**  | ‚úÖ Media a alta       | Media-Alta            | ‚ö°‚ö° Un poco m√°s r√°pida que LSTM |
